{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Harshvardhan, 20D070035, Assignment 3 for EE679"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "# Import the matplotlib.pyplot module for creating plots and visualizations.\n",
    "import numpy as np\n",
    "import os\n",
    "from hmmlearn import hmm\n",
    "import shutil\n",
    "from zipfile import ZipFile\n",
    "# Import the ZipFile class from zipfile for working with zip files.\n",
    "from zipfile import ZipFile\n",
    "# Import various metrics for evaluating machine learning models.\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_zip_file = r'D:\\CA3-ASR.zip'\n",
    "destination_directory = r'D:\\EE679_Assignment3'\n",
    "\n",
    "shutil.unpack_archive(source_zip_file, extract_dir=destination_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source ZIP file on your local machine (Windows)\n",
    "source_zip_file = r'D:\\EE679_Assignment3\\CA3-ASR\\Commands Dataset-20200305T135856Z-001.zip'\n",
    "\n",
    "# Destination directory on your local machine (Windows)\n",
    "destination_directory = r'D:\\EE679_Assignment3\\Assignment3'\n",
    "\n",
    "# Extracting the contents of the ZIP file\n",
    "with ZipFile(source_zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(destination_directory)\n",
    "\n",
    "# Renaming directories as per your requirements\n",
    "original_train_path = os.path.join(destination_directory, 'Commands Dataset', 'train')\n",
    "new_train_zip_path = os.path.join(destination_directory, 'Commands Dataset', 'train_zip')\n",
    "\n",
    "# Check if the destination directory exists before renaming\n",
    "if os.path.exists(new_train_zip_path):\n",
    "    print(f\"Destination directory '{new_train_zip_path}' already exists.\")\n",
    "else:\n",
    "    os.rename(original_train_path, new_train_zip_path)\n",
    "\n",
    "# Navigating to the Commands_Dataset directory\n",
    "os.chdir(os.path.join(destination_directory, 'Commands Dataset'))\n",
    "\n",
    "# Listing the contents of the current directory\n",
    "print(os.listdir())\n",
    "\n",
    "# Unzipping background noise dataset\n",
    "shutil.unpack_archive(\"_background_noise_.zip\", extract_dir=os.path.join(destination_directory, 'Commands Dataset'))\n",
    "\n",
    "# Unzipping test datasets\n",
    "shutil.unpack_archive(\"test_clean.zip\", extract_dir=os.path.join(destination_directory, 'Commands Dataset'))\n",
    "shutil.unpack_archive(\"test_noisy.zip\", extract_dir=os.path.join(destination_directory, 'Commands Dataset'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def extract_and_set_permissions(zip_path, extract_path):\n",
    "    # Check if the destination directory already exists\n",
    "    if not os.path.exists(extract_path):\n",
    "        os.makedirs(extract_path)\n",
    "\n",
    "    # Iterate through files in the ZIP archive\n",
    "    for root, dirs, files in os.walk(zip_path):\n",
    "        for filename in files:\n",
    "            name = filename.split(\".\")[0]\n",
    "            path_zip = os.path.join(zip_path, name)\n",
    "            path_unzip = os.path.join(extract_path, name)\n",
    "\n",
    "            # Check if the file or directory already exists\n",
    "            if not os.path.exists(path_unzip):\n",
    "                # Extract the archive to the destination directory\n",
    "                shutil.unpack_archive(path_zip + \".zip\", extract_dir=path_unzip)\n",
    "\n",
    "                # Set permissions for the extracted files and directories (adjust as needed)\n",
    "                for dirpath, dirnames, filenames in os.walk(path_unzip):\n",
    "                    os.chmod(dirpath, 0o755)  # Example: set directory permissions to rwxr-xr-x\n",
    "                    for filename in filenames:\n",
    "                        filepath = os.path.join(dirpath, filename)\n",
    "                        os.chmod(filepath, 0o644)  # Example: set file permissions to rw-r--r--\n",
    "\n",
    "# Example usage\n",
    "train_zip_path = \"D:\\\\EE679_Assignment3\\\\Assignment3\\\\Commands Dataset\\\\train\"\n",
    "train_path = \"D:\\\\EE679_Assignment3\\\\Assignment3\\\\Commands Dataset\\\\extracted_train\"\n",
    "\n",
    "extract_and_set_permissions(train_zip_path, train_path)\n",
    "print(os.listdir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_read(datapath, dataset_array):\n",
    "    for filename in os.listdir(datapath):\n",
    "        filepath = os.path.join(datapath, filename)\n",
    "\n",
    "        try:\n",
    "            if os.path.isfile(filepath):\n",
    "                sound_file, sr = librosa.load(filepath)\n",
    "                dataset_array.append([sound_file, sr, filename])\n",
    "                print(filepath)\n",
    "            elif os.path.isdir(filepath):\n",
    "                for sample in os.listdir(filepath):\n",
    "                    sample_file_path = os.path.join(filepath, sample)\n",
    "                    try:\n",
    "                        sound_file, sr = librosa.load(sample_file_path)\n",
    "                        dataset_array.append([sound_file, sr, filename])\n",
    "                        print(sample_file_path)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading {sample_file_path}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filepath}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the train_clean directory exists\n",
    "test_clean_path = \"D:\\\\EE679_Assignment3\\\\Assignment3\\\\Commands Dataset\\\\test_clean\"\n",
    "# try:\n",
    "#     os.mkdir(train_clean_path)\n",
    "# except FileExistsError:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the train_noisy directory exists\n",
    "test_noisy_path = \"D:\\\\EE679_Assignment3\\\\Assignment3\\\\Commands Dataset\\\\test_noisy\"\n",
    "# try:\n",
    "#     os.mkdir(train_noisy_path)\n",
    "# except FileExistsError:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_emp(input,k):\n",
    "  duration = len(input) # Calculate the duration (length) of the input signal\n",
    "  output = np.zeros_like(input) # Create an array to store the pre-emphasized signal, initialized with zeros\n",
    "  output[0] = input[0] # Create an array to store the pre-emphasized signal, initialized with zeros\n",
    "  for n in range(1,duration): # Loop through the input signal to perform pre-emphasi\n",
    "    output[n] = input[n] - k*input[n-1]  # Apply the pre-emphasis formula: y[n] = x[n] - k * x[n-1]\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def end_pointing(sound_file, sampling_rate):\n",
    "    # Identify endpoints in the signal based on energy in 20ms window frames\n",
    "\n",
    "    # Calculate the size of the window in samples (20 milliseconds)\n",
    "    window_size = int(sampling_rate * 0.01)\n",
    "\n",
    "    # Divide the sound file into overlapping frames of the specified window size\n",
    "    frames = [sound_file[i * window_size : (i + 2) * window_size] for i in range(len(sound_file) // window_size - 1)]\n",
    "\n",
    "    # Initialize variables for start and end points, noise flag, and update flag\n",
    "    start, end, is_noise, is_updated = 0, 0, True, False\n",
    "\n",
    "    # Calculate the Hamming window to be used for frame energy calculation\n",
    "    hamming_window = np.hamming(window_size * 2)\n",
    "\n",
    "    # Iterate through each frame to identify start and end points\n",
    "    for i, frame in enumerate(frames):\n",
    "        # Calculate energy of the frame using Hamming window\n",
    "        energy = np.matmul(frame * hamming_window, frame * hamming_window)\n",
    "\n",
    "        # Check if energy is above a threshold and update start point\n",
    "        if energy > 0.005:\n",
    "            if is_noise:\n",
    "                is_noise = False\n",
    "                if not is_updated:\n",
    "                    is_updated = True\n",
    "                    start = i\n",
    "\n",
    "        # Check if energy is below a threshold and update end point\n",
    "        if energy < 0.00001 and not is_noise:\n",
    "            is_noise = True\n",
    "            end = i\n",
    "            break\n",
    "\n",
    "    # Calculate the duration and adjust the end point if the duration is too short\n",
    "    duration = end - start\n",
    "    end = len(frames) - 1 if duration < 50 else end\n",
    "\n",
    "    # Extract the portion of the sound file between identified start and end points\n",
    "    new_sound_file = np.array(sound_file[start * window_size : (end + 2) * window_size])\n",
    "\n",
    "    # Return the modified sound file\n",
    "    return new_sound_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mfcc(sound_file, sr):\n",
    "    # Compute MFCC features along with first and second-order deltas\n",
    "\n",
    "    # Compute the MFCC features using librosa with 13 coefficients\n",
    "    mfcc = librosa.feature.mfcc(y=sound_file, sr=sr, n_mfcc=13, dct_type=2, norm='ortho')\n",
    "\n",
    "    # Compute the first-order delta of the MFCC features\n",
    "    mfcc_delta = librosa.feature.delta(mfcc, order=1, mode='nearest')\n",
    "\n",
    "    # Compute the second-order delta of the MFCC features\n",
    "    mfcc_delta2 = librosa.feature.delta(mfcc, order=2, mode='nearest')\n",
    "\n",
    "    # Concatenate the MFCC features, first-order deltas, and second-order deltas along the vertical axis\n",
    "    mfcc_features = np.concatenate((mfcc, mfcc_delta, mfcc_delta2), axis=0)\n",
    "\n",
    "    # Return the concatenated MFCC features\n",
    "    return mfcc_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(test_data, trained_model):\n",
    "    # Calculate the score values for each word in the trained model\n",
    "    score_values = {word: trained_model[word].score(test_data) for word in trained_model.keys()}\n",
    "\n",
    "    # Determine the predicted label based on the word with the maximum score\n",
    "    predicted_label = max(score_values, key=score_values.get)\n",
    "\n",
    "    # Return the predicted label\n",
    "    return predicted_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, title, filename):\n",
    "    # Set the color map for the plot\n",
    "    cmap = plt.cm.YlOrBr\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "\n",
    "    # Set tick marks and labels for axes\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    # Adjust layout for better visualization\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Set labels for axes\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    # Add color bar to the plot\n",
    "    plt.colorbar()\n",
    "\n",
    "    # Save the plot to the specified file\n",
    "    plt.savefig(filename)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-Test begins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = r\"D:\\EE679_Assignment3\\Assignment3\\Commands Dataset\\train file\"\n",
    "labels = [\"down\", \"go\", \"left\", \"no\", \"off\", \"on\", \"right\", \"stop\", \"up\", \"yes\"]\n",
    "# # Extracting contents of train_zip directory to train directory\n",
    "# for root, dirs, files in os.walk(train_zip_path):\n",
    "#     for filename in files:\n",
    "#         name = filename.split(\".\")[0]\n",
    "#         path_zip = os.path.join(train_zip_path, name)\n",
    "#         path_unzip = os.path.join(train_path, name)\n",
    "#         shutil.unpack_archive(path_zip + \".zip\", extract_dir=path_unzip)\n",
    "dataset = {label: [] for label in labels}\n",
    "import wave\n",
    "for label in labels:\n",
    "    folder = os.path.join(train_path, label)\n",
    "    files = [x for x in os.listdir(folder) if x.endswith(\".wav\")][:2000]\n",
    "    for file in files:\n",
    "        dataset[label].append(wav.read(os.path.join(folder, file))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify paths for test_clean, test_noisy, and _background_noise_ directories\n",
    "test_noisy_path = \"D:\\\\EE679_Assignment3\\\\Assignment3\\\\Commands Dataset\\\\test_noisy\"\n",
    "noise_path = r\"D:\\EE679_Assignment3\\Assignment3\\Commands Dataset\\_background_noise_\"\n",
    "\n",
    "# Reading datasets\n",
    "train_dataset = []\n",
    "test_clean_dataset = []\n",
    "test_noisy_dataset = []\n",
    "noise_dataset = []\n",
    "\n",
    "train_test_read(train_path, train_dataset)\n",
    "train_test_read(test_noisy_path, test_noisy_dataset)\n",
    "train_test_read(noise_path, noise_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = {}\n",
    "\n",
    "# Loop through each item in the training dataset\n",
    "for sound_file, sr, word in train_dataset:\n",
    "    # Apply endpointing to the sound file\n",
    "    processed_sound = end_pointing(sound_file, sr)\n",
    "    \n",
    "    # Check if the length of the processed sound is greater than 10000\n",
    "    if len(processed_sound) > 10000:\n",
    "        # Apply pre-emphasis to the processed sound\n",
    "        processed_sound = pre_emp(processed_sound)  \n",
    "\n",
    "        # Compute MFCC features and transpose the result\n",
    "        mfcc_features = mfcc(processed_sound, sr).T\n",
    "        \n",
    "        # Append the MFCC features to the corresponding word in the train_data dictionary\n",
    "        train_data.setdefault(word, []).append(mfcc_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clean_data = {}\n",
    "\n",
    "# Iterate through each word in the test clean path\n",
    "for word in os.listdir(test_clean_path):\n",
    "    test_clean_folder = os.path.join(test_clean_path, word)\n",
    "\n",
    "    # Iterate through each sample in the current word folder\n",
    "    for sample in os.listdir(test_clean_folder):\n",
    "        test_clean_sample_path = os.path.join(test_clean_folder, sample)\n",
    "\n",
    "        # Load the sound file and its sampling rate using librosa\n",
    "        sound_file, sr = librosa.load(test_clean_sample_path)\n",
    "\n",
    "        # Apply endpointing to the sound file\n",
    "        processed_sound = end_pointing(sound_file, sr)\n",
    "\n",
    "        # Check if the length of the processed sound is greater than 10000\n",
    "        if len(processed_sound) > 10000:\n",
    "            # Apply pre-emphasis to the processed sound\n",
    "            processed_sound = pre_emp(processed_sound, k=0.97)  # Provide the value for k\n",
    "\n",
    "            # Compute MFCC features and transpose the result\n",
    "            mfcc_features = mfcc(processed_sound, sr).T\n",
    "\n",
    "            # Append the MFCC features to the corresponding word in the test_clean_data dictionary\n",
    "            test_clean_data.setdefault(word, []).append(mfcc_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_noisy_data = {}\n",
    "\n",
    "# Iterate through each word in the test noisy path\n",
    "for word in os.listdir(test_noisy_path):\n",
    "    test_noisy_folder = os.path.join(test_noisy_path, word)\n",
    "\n",
    "    # Iterate through each sample in the current word folder\n",
    "    for sample in os.listdir(test_noisy_folder):\n",
    "        test_noisy_sample_path = os.path.join(test_noisy_folder, sample)\n",
    "\n",
    "        # Load the sound file and its sampling rate using librosa\n",
    "        sound_file, sr = librosa.load(test_noisy_sample_path)\n",
    "\n",
    "        # Apply endpointing to the sound file\n",
    "        processed_sound = end_pointing(sound_file, sr)\n",
    "\n",
    "        # Check if the length of the processed sound is greater than 10000\n",
    "        if len(processed_sound) > 10000:\n",
    "            # Apply pre-emphasis to the processed sound with k=0.97\n",
    "            processed_sound = pre_emp(processed_sound, k=0.97)\n",
    "\n",
    "            # Compute MFCC features and transpose the result\n",
    "            mfcc_features = mfcc(processed_sound, sr).T\n",
    "\n",
    "            # Append the MFCC features to the corresponding word in the test_noisy_data dictionary\n",
    "            test_noisy_data.setdefault(word, []).append(mfcc_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of samples in train dataset: {len(train_dataset)}\")\n",
    "print(f\"Number of samples in test_clean dataset: {len(test_clean_dataset)}\")\n",
    "print(f\"Number of samples in test_noisy dataset: {len(test_noisy_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = {}\n",
    "hmm_states = 9\n",
    "\n",
    "# Iterate through each word and its corresponding training data\n",
    "for word, current_training_data in train_data.items():\n",
    "    print(word)\n",
    "    \n",
    "    # Initialize a Gaussian HMM model with the specified number of states\n",
    "    model = hmm.GaussianHMM(n_components=hmm_states, covariance_type='full', n_iter=7)\n",
    "    \n",
    "    # Calculate the lengths of each training sequence\n",
    "    lengths = np.array([data.shape[0] for data in current_training_data])\n",
    "    \n",
    "    # Stack the training data vertically to create a single array\n",
    "    current_training_data = np.vstack(current_training_data)\n",
    "    \n",
    "    # Fit the model to the training data with the specified sequence lengths\n",
    "    model.fit(current_training_data, lengths=lengths)\n",
    "    \n",
    "    # Save the trained model for the current word\n",
    "    trained_model[word] = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and labels for test clean data\n",
    "x_test_clean, y_test_clean = zip(*[(feature, word) for word, data in test_clean_data.items() for feature in data]) \n",
    "\n",
    "# Extract features and labels for test noisy data\n",
    "x_test_noisy, y_test_noisy = zip(*[(feature, word) for word, data in test_noisy_data.items() for feature in data]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_clean_data))\n",
    "print(len(test_noisy_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions for clean testing dataset\n",
    "predictions_clean = [prediction(data, trained_model) for data in x_test_clean]\n",
    "\n",
    "# Predictions for noisy testing dataset\n",
    "predictions_noisy = [prediction(data, trained_model) for data in x_test_noisy]\n",
    "\n",
    "# Obtaining the confusion matrix for the clean testing dataset\n",
    "cm_clean = confusion_matrix(y_test_clean, predictions_clean, normalize=\"true\")\n",
    "\n",
    "# Obtaining the confusion matrix for the noisy testing dataset\n",
    "cm_noisy = confusion_matrix(y_test_noisy, predictions_noisy, normalize=\"true\")\n",
    "\n",
    "classes = ['down', 'go', 'left', 'no', 'off', 'on', 'right', 'stop', 'up', 'yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting confusion matrix for clean testing dataset and getting accuracy.\n",
    "title_clean = 'Confusion Matrix for clean test and training data'\n",
    "plot_confusion_matrix(cm_clean, classes, title_clean, \"Clean_test_data.png\")\n",
    "\n",
    "accuracy_clean = np.diag(cm_clean).sum() / cm_clean.sum()\n",
    "print(f'Percentage Accuracy for the trained model tested on the clean test dataset = {accuracy_clean * 100:.2f}%')\n",
    "\n",
    "# Plotting confusion matrix for noisy testing dataset and getting accuracy.\n",
    "title_noisy = 'Confusion Matrix for noisy test and training data'\n",
    "plot_confusion_matrix(cm_noisy, classes, title_noisy, \"Noisy_test_data.png\")\n",
    "\n",
    "accuracy_noisy = np.diag(cm_noisy).sum() / cm_noisy.sum()\n",
    "print(f'Percentage Accuracy for the trained model tested on the noisy test dataset = {accuracy_noisy * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
